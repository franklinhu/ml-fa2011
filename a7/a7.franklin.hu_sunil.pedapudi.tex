\documentclass{ml}
\nameone{Franklin Hu}
\nametwo{Sunil Pedapudi}
\course{CS 194-10 Machine Learning}
\assignment{Assignment 7}

\begin{document}
\maketitle
\newcommand{\pr}{\mathbb{P}}

\begin{question}{Russell \& Norvig 14.7} % 1
    \part % a
\end{question} % 1

\begin{question}{Exponential Family} % 2 
    \part % a
\end{question} % 2

\begin{question}{EM with discrete variables} % 3
    \part % a
\end{question} % 3

\begin{question}{Learning with continuous variables} % 4
    \part % a
        Since \(P(Y_j|X=k)\sim N(\mu_{jk},\sigma^2)\), each element
        of the \(\textbf{Y}\) vector is independent so we can write
        the likelihood of the entire vector as a product of the
        likelihood of its elements. The log-likelihood similarly
        turns into a sum.
       \begin{align*}
            P(\textbf{Y}|X=k)
            &= \prod\limits_{j=1}^D P(Y_j|X=k) \\
            \log P(\textbf{Y}|X=k)
            &= \sum\limits_{j=1}^D \log P(Y_j|X=k)
        \end{align*}
        Given \(N\) i.i.d. observations of \((X,\textbf{Y},Z)\) that we
        call \(E_1, \hdots, E_N\), and \(\textbf{w}=\langle w_1, \hdots,
        w_D \rangle\):
        \begin{align*}
            P(E_1, \hdots, E_N)
            &= \prod\limits_{i=1}^N P(E_i) \\
            &= \prod\limits_{i=1}^N P(X_i=x_i,\textbf{Y}_i=\textbf{y}_i,
                Z_i=z_i) \\
            &= \prod\limits_{i=1}^N P(X_i=x_i) \cdot 
                P(\textbf{Y}_i=\textbf{y}_i|X_i=x_i) \cdot 
                P(Z_i=z_i|X_i=x_i,\textbf{Y}_i=\textbf{y}_i) \\
            &= \prod\limits_{i=1}^N P(X_i=x_i) \cdot 
                P(\textbf{Y}_i=y_i|X_i=x_i) \cdot 
                P(Z_i=z_i|\textbf{Y}_i=\textbf{y}_i) \\
            \log P(E_1, \hdots, E_N)
            &= \sum\limits_{i=1}^N \log P(X_i=x_i) + \log 
                P(\textbf{Y}_i=\textbf{y}_i|X_i=x_i) + \log 
                P(Z_i=z_i|\textbf{Y}_i=\textbf{y}_i) \\
            &= \sum\limits_{i=1}^N \log (0.5) + \left(
                \sum\limits_{j=1}^D \log P(Y_j=y_j|X_i=x_i) \right) + \log 
                P(Z_i=z|\textbf{Y}_i=\textbf{y}_i) \\
            &= \sum\limits_{i=1}^N -\log(2) + \left(
                \sum\limits_{j=1}^D \log \left( 
                \frac{1}{\sigma\sqrt{2\pi}} 
                \exp{\left(\frac{\mu_{j{x_i}}-y_j}{\sigma}\right)^2}
                \right) \right) +
                \log \left( \frac{1}{1+\exp \left(-z_i \textbf{w}^T
                \textbf{y}\right)} \right) \\
            &= -N\log(2) + \sum\limits_{i=1}^N \sum \limits_{j=1}^D
                \left( \left( \frac{\mu_{j{x_i}}-y_j}{\sigma}\right)^2 -
                \log (\sigma\sqrt{2\pi})\right) - 
                \log(1+\exp(-z_i\textbf{w}^T\textbf{y})) \\
            &= -N(\log(2) + D\log(\sigma\sqrt{2\pi})) +
                \sum\limits_{i=1}^N \sum\limits_{j=1}^D \left(
                \frac{\mu_{jx_i}-y_j}{\sigma} \right)^2 - \log
                (1+\exp(-z_i\textbf{w}^T\textbf{y}))
        \end{align*}
        To find the first derivative of the likelihood w.r.t. the
        unknown parameters:
        \begin{align*}
            \frac{\partial}{\partial\sigma}\log \mathcal{L}
            &= -ND\cdot \frac{1}{\sigma\sqrt{2\pi}} \sqrt{2\pi} +
                \sum\limits_{i=1}^N \sum\limits_{j=1}^D -2 \left(
                \frac{(\mu_{jx_i}-y_j)^2}{\sigma^3} \right) \\
            &= -\frac{ND}{\sigma} + \frac{2}{\sigma^3} 
                \sum\limits_{i=1}^N \sum\limits_{j=1}^D
                (\mu_{jx_i}-y_j)^2 \\
            \frac{\partial}{\partial\mu_{jx_i}} \log \mathcal{L}
            &= \sum\limits_{i=1}^N \sum\limits_{j=1}^D
                \frac{2}{\sigma^2} (\mu_{jx_i}-y_j) \\
            &= \frac{2}{\sigma^2} \sum\limits_{i=1}^N
                \sum\limits_{j=1}^D \mu_{jx_i}-y_j
        \end{align*}
\end{question} % 4

\begin{question}{Bayes Net Inference with BayesianLab} % 5
    \part % a
    \part % b
    \part % c
    \part % d
        By giving evidence to the cost nodes (\(PropCost\), \(IliCost\),
        and \(MedCost\)) such that Millions had likelihood 100\%,
        we observed the values of the observable nodes.
        \begin{itemize}
            \item Accident: 99\% Moderate
            \item Antilock: 93\% False
            \item Age: 99\% Adult
            \item SeniorTrain: 99\% False
            \item Mileage: 45.82\% FiftyThou, 36.66\% TwentyThou,
                12.36\% Domino, 5.16\% Fivethou
            \item RuggedAuto: 47.23\% EggShell, 33.74\% Tank,
                19.02\% Football
            \item ThisCarDam: 31.84\% Mild, 25.74\% Moderate, 21.63\%
                None, Severe 20.79\%
            \item Airbag: 84.46\% False
            \item DrivQuality: 37.44\% Excellent, 32.89\% Poor,
                29.68\% Normal
            \item DrivHist: 60.85\% Zero, 27.59\% Many, 11.56\% One
            \item RiskAverasion: 47.39\% Adventurous, 30.50\% 
                Psychopath, 18.05\% Normal, 4.07\% Cautious
            \item VehicleYear: 88.41\% Older, 11.59\% Current
            \item MakeModel: 49.74\% Economy, 33.44\% FamilySedan,
                13.33\% SportsCar, 3.46\% Luxury, 0.03\% SuperLuxury
            \item AntiTheft: 69.32\% False, 30.68\% True
            \item HomeBase: 47.08\% City, 24.89\% Suburb, 18.00\%
                Secure, 10.04\% Rural
            \item GoodStudent: 99.95\% False, 0.05\% True
            \item OtherCar: 64.28\% True, 35.72\% False
        \end{itemize}
    \part % e
        The Bayesian inference files are:
        \begin{itemize}
            \item IliCostBayesian
            \item PropCostBayesian
            \item MedCostBayesian
        \end{itemize}
    \part % f
\end{question} % 5
\end{document}
 
