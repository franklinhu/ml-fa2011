\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{fullpage}
\begin{document}
Franklin Hu, Sunil Pedapudi \\
CS 194-10 Machine Learning \\
Fall 2011 \\
Assignment 4 \\

\begin{enumerate}
    \item Linear neural networks % 1
        \begin{enumerate}
            \item % 1a
                Suppose we have a three layer neural network with one input
                layer \(x\), one hidden layer \(h\), and one output layer 
                \(y\). Each layer can be expressed as a vector of the 
                values of the nodes in that layer. For example,
                \begin{equation*}
                    \bf{y}= \left( \begin{array}{c}
                    y_1 \\
                    y_2 \\
                    \vdots \\
                    y_n \end{array} \right)
                \end{equation*}
                Assume that each neural node has its own set of weights 
                \(\bf{w_i}\) where \(i\) is the node index. We can express 
                the value of a particular output in terms of the hidden 
                layer
                \begin{equation*}
                    y_k= g(\bf{h})
                \end{equation*}
                Since we are only considering linear activation functions,
                we can write this equation in terms of a constant
                multiplied by the weighted sum of inputs
                \begin{equation*}
                    y_k= c_{y_k} \cdot \bf{w_{y_k}} \cdot \bf{h}
                \end{equation*}
                where \(c_{y_k}\) is the constant multiplier of \(y_k\),
                \(w_{y_k}\) is the set of weights for \(y_k\), and \(h\)
                is the vector of hidden nodes. \\
                Similarly, we can express the value of each node in the 
                hidden layer in terms of the inputs.
                \begin{equation*}
                    h_j= c_{h_j} \cdot \bf{w_{h_j}} \cdot \bf{x}
                \end{equation*}
                Now, we can see that the output layer nodes can simply be 
                written in terms of the inputs without the hidden layer.
                For a particular output node:
                \begin{align*}
                    y_k
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot \bf{h} \\
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot
                            \left( \begin{array}{c}
                            h_1 \\
                            h_2 \\
                            \vdots \\
                            h_n 
                            \end{array} \right) \\
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot 
                            \left( \begin{array}{c}
                            {c_h}_1 \cdot \bf{w_{h_1}} \cdot \bf{x} \\
                            {c_h}_2 \cdot \bf{w_{h_2}} \cdot \bf{x} \\
                            \vdots \\
                            {c_h}_n \cdot \bf{w_{h_n}} \cdot \bf{x}
                            \end{array} \right) \\
                        &= c_{y_k} \cdot (\bf{w_{y_k}} \cdot \bf{c_h} \cdot
                            I \cdot \bf{w_h}) \cdot \bf{x}
                \end{align*}
                where \(c_h\) is a vector of the constant weight for each
                hidden node, \(I\) is the identity matrix, and \(w_h\) is
                a matrix of the weight vectors of the hidden nodes.
                Thus we can define a new weight vector \(\bf{u_{y_k}}\) for
                the output node \(y_k\)
                \begin{equation*}
                    {\bf{u_{y_k}}}= \bf{w_{y_k}} \cdot \bf{c_h} \cdot \bf{I}
                    \cdot \bf{w_h}
                \end{equation*}
                We can thus simply compute the value of \(y_k\) in terms of
                \(x\).
            \item % 1b
                For an arbitrary number of hidden nodes, the same
                computation can be done. Suppose we have \(h\) hidden layers
                with \(\bf{h_1}\) having \(\bf{x}\) as inputs and \(\bf{y}\)
                having \(\bf{h_n}\) as inputs. In this case, consider the 
                sub-network of \(\bf{h_2}\), \(\bf{h_1}\), and \(\bf{x}\). 
                Using the technique in part (a), we can write the expression
                for \(\bf{h_2}\) in terms only of \(\bf{x}\).  The resulting
                expression
                \begin{equation*}
                    {\bf{{h_2}_i}}= c_{h_2} \cdot \bf{u_{h_2}} \cdot \bf{x}
                \end{equation*}
                still has a linear activation function and therefore this
                technique generalizes to the remaining hidden nodes. We
                repeat this process for all the hidden nodes and can thus
                write the expression for any output node \(y_k\)
                \begin{equation*}
                    y_k= c_{y_k} \cdot \bf{w_{y_k}} \cdot \left( 
                         \prod\limits_{i=1}^h \bf{c_{h_i}} \cdot \bf{I} 
                         \cdot \bf{w_{h_i}} \right) \cdot \bf{x}
                \end{equation*}
            \item % 1c
                For the case when \(h \ll n\), a neural net with the hidden
                layer will do \(O(hn)\) computations to find the linear
                combination of the weighted sum of inputs whereas without
                the hidden layer, as shown in (a), the output is only
                dependent on \(x\). This computations is \(O(n)\), so we 
                save those \(h-1\) other computations over the inputs.
        \end{enumerate}
    \item ML estimation of exponential model \\ % 2
        Knowing
        \begin{equation*}
            P(x) = \frac{1}{b}e^{-\frac{x}{b}}
        \end{equation*}
        \begin{enumerate}
            \item % 2a
                We write the likelihood function given \(x_i\) as
                \begin{align*}
                \mathcal{L}(b) 
                &= \prod_{i = 1}^N \frac{1}{b}e^{-\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N \prod_{i=1}^N e^{-\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N e^{\sum_{i=0}^N -\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N exp(-\frac{1}{b}\sum_{i=0}^N x_i)\\
                \textnormal{Let } \bar{x} &= \frac{1}{N}\sum_{i=1}^Nx_i,\\
                \mathcal{L}(b) &= \left(\frac{1}{b}\right)^N exp(-\frac{1}{b}N\bar{x})
                \end{align*}

            \item %pp 2b
                We first find
                \begin{align*}
                log(\mathcal{L}) 
                    &= log\left(\left(\frac{1}{b}\right)^N exp(-\frac{1}{b}N\bar{x})\right)\\
                    &= log\left(\frac{1}{b}\right)^N + 
                        log\left(e^{-\frac{1}{b}N\bar{x}}\right)\\
                    &= N(log(1) - log(b)) -\frac{1}{b}N\bar{x}\\
                    &= -Nlog(b) - \frac{1}{b}N\bar{x}
                \end{align*}

                Then, let \(\theta = \frac{1}{b}\), the parameter variable
                \begin{align*}
                    \frac{\partial log(\mathcal{L}(\theta))}{\partial \theta}
                    &= \frac{\partial Nlog(\theta)}{\partial \theta} - 
                    \frac{\partial \theta N\bar{x}}{\partial \theta}\\
                    &= \frac{N}{\theta} - \frac{\partial \theta N\bar{x}}{\partial \theta}\\
                    &= \frac{N}{\theta} - N\bar{x}\\
                    &= Nb - N\bar{x}
                \end{align*}

            \item % 2c
                We aim to maximize \(\mathcal{L}\) so,
                \begin{equation*}
                    \frac{\partial \mathcal{L}}{\partial \theta} = 
                    Nb - N\bar{x} = 0
                \end{equation*}
                We can solve this to find 
                \begin{equation*}
                  b = \bar{x} = \frac{1}{N}\sum_{i=0}^Nx_i
                \end{equation*}
        \end{enumerate}

    \item ML estimation of noisy-OR model
\end{enumerate}

\end{document}
