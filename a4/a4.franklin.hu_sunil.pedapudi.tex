\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{fullpage}
\begin{document}
Franklin Hu, Sunil Pedapudi \\
CS 194-10 Machine Learning \\
Fall 2011 \\
Assignment 4 \\

\begin{enumerate}
    \item Linear neural networks % 1
        \begin{enumerate}
            \item % 1a
                Suppose we have a three layer neural network with one input
                layer \(x\), one hidden layer \(h\), and one output layer 
                \(y\). Each layer can be expressed as a vector of the 
                values of the nodes in that layer. For example,
                \begin{equation*}
                    \bf{y}= \left( \begin{array}{c}
                    y_1 \\
                    y_2 \\
                    \vdots \\
                    y_n \end{array} \right)
                \end{equation*}
                Assume that each neural node has its own set of weights 
                \(\bf{w_i}\) where \(i\) is the node index. We can express 
                the value of a particular output in terms of the hidden 
                layer
                \begin{equation*}
                    y_k= g(\bf{h})
                \end{equation*}
                Since we are only considering linear activation functions,
                we can write this equation in terms of a constant
                multiplied by the weighted sum of inputs
                \begin{equation*}
                    y_k= c_{y_k} \cdot \bf{w_{y_k}} \cdot \bf{h}
                \end{equation*}
                where \(c_{y_k}\) is the constant multiplier of \(y_k\),
                \(w_{y_k}\) is the set of weights for \(y_k\), and \(h\)
                is the vector of hidden nodes. \\
                Similarly, we can express the value of each node in the 
                hidden layer in terms of the inputs.
                \begin{equation*}
                    h_j= c_{h_j} \cdot \bf{w_{h_j}} \cdot \bf{x}
                \end{equation*}
                Now, we can see that the output layer nodes can simply be 
                written in terms of the inputs without the hidden layer.
                For a particular output node:
                \begin{align*}
                    y_k
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot \bf{h} \\
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot
                            \left( \begin{array}{c}
                            h_1 \\
                            h_2 \\
                            \vdots \\
                            h_n 
                            \end{array} \right) \\
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot 
                            \left( \begin{array}{c}
                            {c_h}_1 \cdot \bf{w_{h_1}} \cdot \bf{x} \\
                            {c_h}_2 \cdot \bf{w_{h_2}} \cdot \bf{x} \\
                            \vdots \\
                            {c_h}_n \cdot \bf{w_{h_n}} \cdot \bf{x}
                            \end{array} \right) \\
                        &= c_{y_k} \cdot (\bf{w_{y_k}} \cdot \bf{c_h} \cdot
                            I \cdot \bf{w_h}) \cdot \bf{x}
                \end{align*}
                where \(c_h\) is a vector of the constant weight for each
                hidden node, \(I\) is the identity matrix, and \(w_h\) is
                a matrix of the weight vectors of the hidden nodes.
                Thus we can define a new weight vector \(\bf{u_{y_k}}\) for
                the output node \(y_k\)
                \begin{equation*}
                    {\bf{u_{y_k}}}= \bf{w_{y_k}} \cdot \bf{c_h} \cdot \bf{I}
                    \cdot \bf{w_h}
                \end{equation*}
                We can thus simply compute the value of \(y_k\) in terms of
                \(x\).
            \item % 1b
                For an arbitrary number of hidden nodes, the same
                computation can be done. Suppose we have \(h\) hidden layers
                with \(\bf{h_1}\) having \(\bf{x}\) as inputs and \(\bf{y}\)
                having \(\bf{h_n}\) as inputs. In this case, consider the 
                sub-network of \(\bf{h_2}\), \(\bf{h_1}\), and \(\bf{x}\). 
                Using the technique in part (a), we can write the expression
                for \(\bf{h_2}\) in terms only of \(\bf{x}\).  The resulting
                expression
                \begin{equation*}
                    {\bf{{h_2}_i}}= c_{h_2} \cdot \bf{u_{h_2}} \cdot \bf{x}
                \end{equation*}
                still has a linear activation function and therefore this
                technique generalizes to the remaining hidden nodes. We
                repeat this process for all the hidden nodes and can thus
                write the expression for any output node \(y_k\)
                \begin{equation*}
                    y_k= c_{y_k} \cdot \bf{w_{y_k}} \cdot \left( 
                         \prod\limits_{i=1}^h \bf{c_{h_i}} \cdot \bf{I} 
                         \cdot \bf{w_{h_i}} \right) \cdot \bf{x}
                \end{equation*}
            \item % 1c
                For the case when \(h \ll n\), a neural net with the hidden
                layer will do \(O(hn)\) computations to find the linear
                combination of the weighted sum of inputs whereas without
                the hidden layer, as shown in (a), the output is only
                dependent on \(x\). This computations is \(O(n)\), so we 
                save those \(h-1\) other computations over the inputs.
        \end{enumerate}
    \item ML estimation of exponential model \\ % 2
        Knowing
        \begin{equation*}
            P(x) = \frac{1}{b}e^{-\frac{x}{b}}
        \end{equation*}
        \begin{enumerate}
            \item % 2a
                We write the likelihood function given \(x_i\) as
                \begin{align*}
                \mathcal{L}(b) 
                &= \prod_{i = 1}^N \frac{1}{b}e^{-\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N \prod_{i=1}^N e^{-\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N e^{\sum_{i=0}^N -\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N exp(-\frac{1}{b}\sum_{i=0}^N x_i)\\
                \textnormal{Let } \bar{x} &= \frac{1}{N}\sum_{i=1}^Nx_i,\\
                \mathcal{L}(b) &= \left(\frac{1}{b}\right)^N exp(-\frac{1}{b}N\bar{x})
                \end{align*}

            \item %pp 2b
                We first find
                \begin{align*}
                log(\mathcal{L}) 
                    &= log\left(\left(\frac{1}{b}\right)^N exp(-\frac{1}{b}N\bar{x})\right)\\
                    &= log\left(\frac{1}{b}\right)^N + 
                        log\left(e^{-\frac{1}{b}N\bar{x}}\right)\\
                    &= N(log(1) - log(b)) -\frac{1}{b}N\bar{x}\\
                    &= -Nlog(b) - \frac{1}{b}N\bar{x}
                \end{align*}

                Then, let \(\theta = \frac{1}{b}\), the parameter variable
                \begin{align*}
                    \frac{\partial log(\mathcal{L}(\theta))}{\partial \theta}
                    &= \frac{\partial Nlog(\theta)}{\partial \theta} - 
                    \frac{\partial \theta N\bar{x}}{\partial \theta}\\
                    &= \frac{N}{\theta} - \frac{\partial \theta N\bar{x}}{\partial \theta}\\
                    &= \frac{N}{\theta} - N\bar{x}\\
                    &= Nb - N\bar{x}
                \end{align*}

            \item % 2c
                We aim to maximize \(\mathcal{L}\) so,
                \begin{equation*}
                    \frac{\partial \mathcal{L}}{\partial \theta} = 
                    Nb - N\bar{x} = 0
                \end{equation*}
                We can solve this to find 
                \begin{equation*}
                  b = \bar{x} = \frac{1}{N}\sum_{i=0}^Nx_i
                \end{equation*}
        \end{enumerate}

    \item ML estimation of noisy-OR model % 3
    
    \item Naive Bayes models for spam filtering % 4
        \begin{enumerate}
            \item % 4a
            \item % 4b
                We modified the skeleton code in NBmodel.py. We were not
                sure what the point was to have each NaiveBayesModel have
                a pointer to another model and just made this base class the
                model used. Most of the training and classification code was
                common between the Boolean and NTF subclasses with the
                exception of the classification code to calculate the log
                probabilities. \\
                In general, we calcuate the log probability ratio:
                \begin{equation*}
                    \frac{P(Y=1|X)}{P(Y=0|X)} \rightarrow
                    \text{log}\frac{P(Y=1|X)}{P(Y=0|X)} 
                \end{equation*}
                If the original ratio is greater than 1, then we classify
                the example as spam. If it is less than 1, we classify it 
                as ham. If it is equal to one, we flip a fair coin. \\
                When we take the log, we simply adjust the value we compare
                it to since \(log1=0\). \\
                For Boolean features, our expression for the likelihood is
                \begin{align*}
                    P(Y=1|X)
                        &= P(Y=1) \cdot P(X|Y=1) \\
                        &= P(Y=1) \cdot \prod\limits_{i=1}^n P(x_i|Y=1) \\
                        &= \alpha \theta_1 \prod\limits_{i=1}^n 
                            {\theta_{1i}}^{x_i}(1-\theta_{1i})^{1-x_i}
                \end{align*}
                Since we assume that the attributes are independent, we can
                turn \(P(X|Y=1)\) into a product of the individual
                probabilities. \\
                When we take the log ratio, we get
                \begin{align*}
                    \frac{P(Y=1|X)}{P(Y=0|X)}
                        &= \frac
                            {\alpha \theta_1 
                                \prod\limits_{i=1}^n {\theta_{1i}}^{x_i}
                                (1-\theta_{1i})^{1-x_i}}
                            {\alpha \theta_0
                                \prod\limits_{i=1}^n {\theta_{0i}}^{x_i}
                                (1-\theta_{0i})^{1-x_i}} \\
                        &= \frac
                            {\theta_1 
                                \prod\limits_{i=1}^n {\theta_{1i}}^{x_i}
                                (1-\theta_{1i})^{1-x_i}}
                            {\theta_0
                                \prod\limits_{i=1}^n {\theta_{0i}}^{x_i}
                                (1-\theta_{0i})^{1-x_i}} \\
                    \text{log} \frac{P(Y=1|X)}{P(Y=0|X)}
                        &= \text{log} \left( \theta_1 
                                \prod\limits_{i=1}^n {\theta_{1i}}^{x_i}
                                (1-\theta_{1i})^{1-x_i} \right) -
                           \text{log} \left( \theta_0
                                \prod\limits_{i=1}^n {\theta_{0i}}^{x_i}
                                (1-\theta_{0i})^{1-x_i} \right) \\
                        &= \text{log}\theta_1 - \text{log}\theta_0 + 
                                \sum\limits_{i=1}^n 
                                    x_i \cdot \text{log}\theta_{1i} + 
                                    (1-x_i) \cdot \text{log}(1-\theta_{1i})
                                    -
                                \sum\limits_{i=1}^n 
                                    x_i \cdot \text{log}\theta_{0i} -
                                    (1-x_i) \cdot \text{log}(1-\theta_{0i})
                \end{align*}
                For NTF, we can make the same independence assumption and
                derive a log probability ratio
                \begin{align*}
                    \frac{P(Y=1|X)}{P(Y=0|X)}
                    &= \frac{P(Y=1) \cdot P(X|Y=1)}{P(Y=0) \cdot P(X|Y=0)}
                        \\
                    &= \frac
                        {P(Y=1) \cdot \prod\limits_{i=1}^n P(x_i|Y=1}
                        {P(Y=0) \cdot \prod\limits_{i=1}^n P(x_i|Y=0} \\
                    &= \frac
                        {\alpha \theta_1 \prod\limits_{i=1}^n
                            \frac{1}{b_i} e^{-\frac{x_i}{b_i}}}
                        {\alpha \theta_0 \prod\limits_{j=1}^n
                            \frac{1}{b_j} e^{-\frac{x_j}{b_j}}} \\
                    &= \frac
                        {\theta_1 \prod\limits_{i=1}^n
                            \frac{1}{b_i} e^{-\frac{x_i}{b_i}}}
                        {\theta_0 \prod\limits_{j=1}^n
                            \frac{1}{b_j} e^{-\frac{x_i}{b_j}}} \\
                    \text{log} \frac{P(Y=1|X)}{P(Y=0|X)}
                    &= \text{log} \frac
                        {\theta_1 \prod\limits_{i=1}^n
                            \frac{1}{b_i} e^{-\frac{x_i}{b_i}}}
                        {\theta_0 \prod\limits_{j=1}^n
                            \frac{1}{b_j} e^{-\frac{x_i}{b_j}}} \\
                    &= \text{log} \left( \theta_1 \prod\limits_{i=1}^n
                            \frac{1}{b_i} e^{-\frac{x_i}{b_i}} \right) -
                        \text{log} \left( \theta_0 \prod\limits_{j=1}^n
                            \frac{1}{b_j} e^{-\frac{x_i}{b_j}} \right) \\
                    &= \text{log} \theta_1 - \text{log} \theta_0 +
                        \sum\limits_{i=1}^n \text{log} \left( \frac{1}{b_i} 
                            e^{-\frac{x_i}{b_i}} \right) -
                        \sum\limits_{j=1}^n \text{log} \left( \frac{1}{b_j}
                            e^{-\frac{x_i}{b_j}} \right) \\
                    &=  \text{log} \theta_1 - \text{log} \theta_0 +
                        \left( \sum\limits_{i=1}^n -\text{log} b_i - 
                            \frac{x_i}{b_i} \right)
                         - \left( \sum\limits_{j=1}^n - \text{log} b_j - 
                            \frac{x_j}{b_j} \right)
                \end{align*}
                where \(b_i\) and \(b_j\) are the appropriate constants
                calucated with the closed form solution in question 2 for
                each feature.
        \end{enumerate}
\end{enumerate}

\end{document}
