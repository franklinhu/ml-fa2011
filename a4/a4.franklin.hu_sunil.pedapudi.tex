\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\begin{document}
Franklin Hu, Sunil Pedapudi \\
CS 194-10 Machine Learning \\
Fall 2011 \\
Assignment 4 \\

\begin{enumerate}
    \item Linear neural networks % 1
        \begin{enumerate}
            \item % 1a
                Suppose we have a three layer neural network with one input
                layer \(x\), one hidden layer \(h\), and one output layer 
                \(y\). Each layer can be expressed as a vector of the 
                values of the nodes in that layer. For example,
                \begin{equation}
                    \bf{y}= \left( \begin{array}{c}
                    y_1 \\
                    y_2 \\
                    \vdots \\
                    y_n \end{array} \right)
                \end{equation}
                Assume that each neural node has its own set of weights 
                \(\bf{w_i}\) where \(i\) is the node index. We can express 
                the value of a particular output in terms of the hidden 
                layer
                \begin{equation}
                    y_k= c_{y_k} \cdot \bf{w_{y_k}} \cdot \bf{h}
                \end{equation}
                where \(c_{y_k}\) is the constant multiplier of \(y_k\),
                \(w_{y_k}\) is the set of weights for \(y_k\), and \(h\)
                is the vector of hidden nodes. \\
                Similarly, we can express the value of each node in the 
                hidden layer in terms of the inputs.
                \begin{equation}
                    h_j= c_{h_j} \cdot \bf{w_{h_j}} \cdot \bf{x}
                \end{equation}
                Now, we can see that the output layer nodes can simply be 
                written in terms of the inputs without the hidden layer.
                For a particular output node:
                \begin{align*}
                    y_k
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot \bf{h} \\
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot
                            \left( \begin{array}{c}
                            h_1 \\
                            h_2 \\
                            \vdots \\
                            h_n 
                            \end{array} \right) \\
                        &= c_{y_k} \cdot \bf{w_{y_k}} \cdot 
                            \left( \begin{array}{c}
                            {c_h}_1 \cdot \bf{w_{h_1}} \cdot \bf{x} \\
                            {c_h}_2 \cdot \bf{w_{h_2}} \cdot \bf{x} \\
                            \vdots \\
                            {c_h}_n \cdot \bf{w_{h_n}} \cdot \bf{x}
                            \end{array} \right) \\
                        &= c_{y_k} \cdot 
                            \left( \begin{array}{cccc}
                            w_{y_{k_1}} & w_{y_{k_2}} & \hdots & w_{y_{k_n}}
                            \end{array} \right)
                            \left( \begin{array}{c}
                                c_{h_1} \cdot \bf{w_{h_1}} \cdot \bf{x} \\
                                c_{h_2} \cdot \bf{w_{h_2}} \cdot \bf{x} \\
                                \vdots \\
                                c_{h_n} \cdot \bf{w_{h_n}} \cdot \bf{x}
                            \end{array} \right) \\
                        &= c_{y_k} \sum\limits_{i=1}^n {w_{y_k}}_i \cdot {c_h}_i 
                            \cdot \bf{{w_h}_i} \cdot \bf{x} \\
                        &= c_{y_k} \left( \sum\limits_{i=1}^n {w_{y_k}}_i 
                            \cdot {c_h}_i \cdot \bf{{w_h}_i} \right) \bf{x}
                \end{align*}
            \item % 1b
                For an arbitrary number of hidden nodes, the same
                computation can be done. We demonstrate below with two
                hidden layers: \(h_m, h_n\)
            \item % 1c
                For the case when \(h \ll n\), a neural net with the hidden
                layer will do \(O(hn)\) computations to find the linear
                combination of the weighted sum of inputs whereas without
                the hidden layer, as shown in (a), the output is only
                dependent on \(x\). This computations is \(O(n)\), so we 
                save those \(h-1\) other computations over the inputs.
        \end{enumerate}
    \item ML estimation of exponential model \\ % 2
        Knowing
        \begin{equation*}
            P(x) = \frac{1}{b}e^{-\frac{x}{b}}
        \end{equation*}
        \begin{enumerate}
            \item % 2a
                We write the likelihood function given \(x_i\) as
                \begin{align*}
                \mathcal{L}(b|x_1,\hdots,x_N) 
                &= \prod_{i = 1}^N \frac{1}{b}e^{-\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N \prod_{i = 1}^N e^{-\frac{x_i}{b}}\\
                &= \left(\frac{1}{b}\right)^N e^{\sum_{i=0}^N \frac{x_i}{b}}
                \end{align*}

            \item % 2b
                We first find
                \begin{align*}
                log(\mathcal{L}) 
                    &= log\left(\left(\frac{1}{b}\right)^N e^{\sum_{i=0}^N 
                        \frac{x_i}{b}}\right)\\
                    &= log\left(\left(\frac{1}{b}\right)^N\right) + 
                        log\left(e^{\sum_{i=0}^N \frac{x_i}{b}}\right)\\
                    &= n(log(1) - log(b)) + \sum_{i=0}^N \frac{x_i}{b}log(e)
                \end{align*}

                Then,
                \begin{align*}
                    \frac{\partial log \mathcal{L}}{\partial b}
                    &= \frac{\partial N(log(1) - log(b))}{\partial b} + 
                        \frac{\partial \sum_{i=0}^N 
                        \frac{x_i}{b}log(e)}{\partial b}\\
                    &= -\frac{N}{b} + \frac{\partial \frac{N}{b}
                        \sum_{i=0}^N x_i\cdot log(e)}{\partial b}\\
                    &= -\frac{N}{b} - \frac{N}{b^2}\sum_{i=0}^N x_i\cdot 
                        log(e)\\
                    &= -\frac{N}{b}\left(1-\frac{1}{b}\sum_{i=0}^N x_i 
                        log(e)\right)
                \end{align*}

            \item % 2c
                We aim to maximize \(\mathcal{L}\) so,
                \begin{equation*}
                    \frac{\partial \mathcal{L}}{\partial b} = 
                        -\frac{N}{b}\left(1-\frac{1}{b}\sum_{i=0}^N x_i 
                        log(e)\right) = 0
                \end{equation*}
                We can reassemble this as
                \begin{align*}
                    -\frac{N}{b}\left(1-\frac{1}{b}log(e)\sum_{i=0}^N x_i
                        \right) &= 0\\
                    -N + \frac{N}{b}log(e)\sum_{i=0}^N x_i &= 0\\
                    \frac{N}{b}log(e)\sum_{i=0}^N x_i &= N\\
                    Nlog(e)\sum_{i=0}^N x_i &= Nb\\
                    log(e)\sum_{i=0}^N x_i &= b
                \end{align*}
        \end{enumerate}

    \item ML estimation of noisy-OR model
\end{enumerate}

\end{document}
