\documentclass{ml}
\nameone{Franklin Hu}
\nametwo{Sunil Pedapudi}
\course{CS 194-10 Machine Learning}
\assignment{Assignment 5}

\begin{document}
\maketitle
\newcommand{\pr}{\mathbb{P}}

\begin{question}{Density estimation in one dimension} % 1
  \part % a
  Given
  \begin{equation*}
    \hat{P}(x) = \frac{1}{N}\sum_{i=1}^NK_b(d(x,x_i))
  \end{equation*}
  we wish to show that
  \begin{equation*}
    \int \frac{1}{N}\sum_{i=1}^NK_b(d(x,x_i))\,dx = 1
  \end{equation*}
  Let us acknowledge that integrating with respect to \(x\) will exercise
  all values across the range of \(d(x,x_i)\) which, in turn, exercises the
  range of \(K_b\). Therefore, we simplify the integrand to be \(K_b(z)\)
  where \(z= d(x,x_i)\) and integrate with respect to \(z\).
  We then evaluate
  \begin{align*}
    \int \frac{1}{N}\sum_{i=1}^NK_b(z)\,dz 
    &= \frac{1}{N} \int \sum_{i=1}^NK_b(z)\,dz\\
    &= \frac{1}{N} \sum_{i=1}^N \int K_b(z)\,dz\\
    &= \frac{1}{N} \sum_{i=1}^N 1\\
    &= 1
  \end{align*}
  \part % b
  We assume that \(N=1\) for the sake of this exercise and work with
  \begin{equation*}
    \hat{P}(x) = \frac{1}{2d_k(x)}
  \end{equation*}
  We wish to show
  \begin{equation*}
    \int \frac{1}{2d_k(x)}\,dx \neq 1
  \end{equation*}
  Like in part a, we observe that integrating with respect to \(x\) will
  exercise the range of \(d_k(x)\), so we substitute \(z\) in place of
  \(d_k(x)\) and thus find
  \begin{align*}
    \int \frac{1}{2z}\,dz \\
    &= \frac{1}{2} \int \frac{1}{z}\,dz\\
    &= \frac{1}{2} log(z)|_{-\infty}^{\infty}\\
    &\neq 1
  \end{align*}
  since \(log(z)|_{-\inf}^{\inf}\) is divergent. Since this component of the
  integrand persists regardless of the value of \(N\), we can generalize that
  this function is not a proper density.
  \part % c
  \part % d
\end{question} % 1

\begin{question}{EM} % 2
  \part[]
\end{question} % 2

\begin{question}{Density estimation for seismic data} % 3
  \part % a
    Given the Laplacian kernel
    \begin{equation*}
      K_b(d) = \frac{1}{b} \exp \left( -\frac{d}{b} \right)
    \end{equation*}
    Suppose we have a training set of size \(N\) and a validation
    (held out) set of size \(M\). \\
    The log-likelihood of the held out data is
    \begin{align*}
      P(x_1,x_2,\hdots,x_M)  
        &= \prod_{i=1}^M P(x_i) \\
      \log P(x_1,x_2,\hdots,x_M) 
        &= \log \prod\limits_{i=1}^M P(x_i) \\
        &= \sum\limits_{i=1}^M \log P(x_i)
    \end{align*}
    The log-likelihood of each individual validation query point can be
    derived for kernel densities (a), (c), and (d) from the following 
    common expression with a Laplacian kernel
    \begin{align*}
      \hat{P}(x) 
        &= \frac{1}{N} \sum\limits_{i=1}^N K_b(d(x,x_i)) \\
      \log \hat{P}(x)
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
           K_b(d(x,x_i)) \right) \\
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
           \frac{1}{b} \exp\left(-\frac{d(x,x_i)}{b}\right) \right)
    \end{align*}
    For kernel density (b), the log-likelihood is
    \begin{align*}
      \hat{P}(x) &= \frac{k}{2Nd_k(k)} \\
      \log{\hat{P}(x)} &= \log{k} - \log{2Nd_k(k)}
    \end{align*}
  \part % b
    \item
  \part % c
    \item
\end{question}

\end{document}
 
