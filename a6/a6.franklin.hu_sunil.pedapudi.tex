\documentclass{ml}
\nameone{Franklin Hu}
\nametwo{Sunil Pedapudi}
\course{CS 194-10 Machine Learning}
\assignment{Assignment 5}

\begin{document}
\maketitle
\newcommand{\pr}{\mathbb{P}}

\begin{question}{Density estimation in one dimension} % 1
  \part % a
  \part % b
  \part % c
  \part % d
\end{question} % 1

\begin{question}{EM} % 2
  \part[]
\end{question} % 2

\begin{question}{Density estimation for seismic data} % 3
  \part % a
    Suppose we have a training set of size \(N\) and a validation
    (held out) set of size \(M\). \\
    The log-likelihood of the held out data is
    \begin{align*}
      P(x_1,x_2,\hdots,x_M)  
        &= \prod_{i=1}^M P(x_i) \\
      \log P(x_1,x_2,\hdots,x_M) 
        &= \log \prod\limits_{i=1}^M P(x_i) \\
        &= \sum\limits_{i=1}^M \log P(x_i)
    \end{align*}
    The log-likelihood of each individual validation query point can be
    derived for kernel densities (a), (c), and (d) from the following 
    common expression with a Laplacian kernel
    \begin{align*}
      \hat{P}(x) 
        &= \frac{1}{N} \sum\limits_{i=1}^N K_b(d(x,x_i)) \\
      \log \hat{P}(x)
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
           K_b(d(x,x_i)) \right) \\
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
           \exp\left(-\frac{d(x,x_i)}{b}\right) \right)
    \end{align*}
    Using Jensen's inequality: Given \(\sum_j \lambda_j=1\), 
    \(\lambda_j>0\), and a concave function \(f\):
    \begin{equation*}
      f(\sum_j \lambda_j \cdot x_j) \ge \sum_j \lambda_j \cdot f(x_j)
    \end{equation*}
    Since log is concave, we can simply let our \(\lambda_j=
    \frac{1}{N}\) and thus our log-likehood equation simplifies to
    \begin{align*}
       \log \hat{P}(x)
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
          \exp \left( -\frac{d(x,x_i)}{b} \right) \right) \\
        &\ge \frac{1}{N} \sum\limits_{i=1}^{N} -\frac{d(x,x_i)}{b} \\
        &= -\frac{1}{N} \sum\limits_{i=1}^N \frac{d(x,x_i)}{b}
    \end{align*}
    For kernel densities (c) and (d), \(b\) is simply a function of
    the training or validation data. \\
    For kernel density (b), the log-likelihood is
    \begin{align*}
      \hat{P}(x) &= \frac{k}{2Nd_k(k)} \\
      \log{\hat{P}(x)} &= \log{k} - \log{2Nd_k(k)}
    \end{align*}
  \part % b
    \item
  \part % c
    \item
\end{question}

\end{document}
 
