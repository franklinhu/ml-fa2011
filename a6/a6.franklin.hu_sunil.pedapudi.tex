\documentclass{ml}
\nameone{Franklin Hu}
\nametwo{Sunil Pedapudi}
\course{CS 194-10 Machine Learning}
\assignment{Assignment 5}

\begin{document}
\maketitle
\newcommand{\pr}{\mathbb{P}}

\begin{question}{Density estimation in one dimension} % 1
  \part % a
  Given
  \begin{equation*}
    \hat{P}(x) = \frac{1}{N}\sum_{i=1}^NK_b(d(x,x_i))
  \end{equation*}
  we wish to show that
  \begin{equation*}
    \int \frac{1}{N}\sum_{i=1}^NK_b(d(x,x_i))\,dx = 1
  \end{equation*}
  Let us acknowledge that integrating with respect to \(x\) will exercise
  all values across the domain of \(d(x,x_i)\) which, in turn, exercises the
  range of \(K_b\). Therefore, we simplify the integrand to be \(K_b(z)\)
  where \(z= d(x,x_i)\) and integrate with respect to \(z\).
  We then evaluate
  \begin{align*}
    \int \frac{1}{N}\sum_{i=1}^NK_b(z)\,dz 
    &= \frac{1}{N} \int \sum_{i=1}^NK_b(z)\,dz\\
    &= \frac{1}{N} \sum_{i=1}^N \int K_b(z)\,dz\\
    &= \frac{1}{N} \sum_{i=1}^N 1\\
    &= 1
  \end{align*}
  \part % b
  We assume that \(N=1\) for the sake of this exercise and work with
  \begin{equation*}
    \hat{P}(x) = \frac{1}{2d_k(x)}
  \end{equation*}
  We wish to show
  \begin{equation*}
    \int \frac{1}{2d_k(x)}\,dx \neq 1
  \end{equation*}
  Like in part a, we observe that integrating with respect to \(x\) will
  exercise the range of \(d_k(x)\), so we substitute \(z\) in place of
  \(d_k(x)\) and thus find
  \begin{align*}
    \int \frac{1}{2z}\,dz \\
    &= \frac{1}{2} \int \frac{1}{z}\,dz\\
    &= \frac{1}{2} log(z)|_{-\infty}^{\infty}\\
    &\neq 1
  \end{align*}
  since \(log(z)|_{-\inf}^{\inf}\) is divergent. Since this component of the
  integrand persists regardless of the value of \(N\), we can generalize that
  this function is not a proper density.
  \part % c
  Given,
  \begin{align*}
    \hat{P}(x) &= \frac{1}{N}\sum_{i=1}^N K_{d_k(x)}(d(x,x_i))\\
    &= \frac{1}{N}\sum_{i=1}^N ae^{-\frac{x-x_i}{2(d(x,x_i))}}
  \end{align*}
  it is evident that 
  \begin{equation*}
    \frac{1}{N} \int \sum_{i=1}^N ae^{-\frac{x-x_i}{2(d(x,x_i))}}\,dx \neq 1
  \end{equation*}
  because \(ae^{-\frac{x-x_i}{2(d(x,x_i))}}\) fails to describe a proper
  Gaussian function since \(\sigma\) constantly varies across \(x\). 
  \part % d
  Let us first recognize that
  \begin{equation*}
    \frac{1}{N} \int \sum_{i=1}^N K_{d_{ik}}(d(x,x_i)) = 1
  \end{equation*}
  since \(d_{ik}\) is constant with respect to \(x_i\) so integrating with
  respect to \(x\) renders a sum of integrals of \(N\) kernels. Normalizing
  this sum shows that this is indeed a proper kernel density function. It
  differs from \(k\)-NN density estimator in that data points that are 
  isolated (i.e. its nearest neighbors are far away) are given little
  preference in calculating the likelihood of a query point.
  
\end{question} % 1

\begin{question}{EM} % 2
  \part % a
  Essentially, if there were only two attributes, then EM will try to 
  classify the candy per attribute pairs into Bag 1 or Bag 2 with 
  some \(\theta\) rather than attempt to retrieve the original
  distribution. 

  \part % b
  
\end{question} % 2

\begin{question}{Density estimation for seismic data} % 3
  \part % a
    Given the Laplacian kernel
    \begin{equation*}
      K_b(d) = \frac{1}{b} \exp \left( -\frac{d}{b} \right)
    \end{equation*}
    Suppose we have a training set of size \(N\) and a validation
    (held out) set of size \(M\). \\
    The log-likelihood of the held out data is
    \begin{align*}
      P(x_1,x_2,\hdots,x_M)  
        &= \prod_{i=1}^M P(x_i) \\
      \log P(x_1,x_2,\hdots,x_M) 
        &= \log \prod\limits_{i=1}^M P(x_i) \\
        &= \sum\limits_{i=1}^M \log P(x_i)
    \end{align*}
    The log-likelihood of each individual validation query point can be
    derived for kernel densities (a), (c), and (d) from the following 
    common expression with a Laplacian kernel
    \begin{align*}
      \hat{P}(x) 
        &= \frac{1}{N} \sum\limits_{i=1}^N K_b(d(x,x_i)) \\
      \log \hat{P}(x)
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
           K_b(d(x,x_i)) \right) \\
        &= \log \left( \frac{1}{N} \sum\limits_{i=1}^N
           \frac{1}{b} \exp\left(-\frac{d(x,x_i)}{b}\right) \right)
    \end{align*}
    For kernel density (b), the log-likelihood is
    \begin{align*}
      \hat{P}(x) &= \frac{k}{2Nd_k(k)} \\
      \log{\hat{P}(x)} &= \log{k} - \log{2Nd_k(k)}
    \end{align*}
    Experimentally, we found it was infeasible to run it over a large
    portion of the data set. We decided to filter the data to only use
    examples that were within 15 degrees longitude of the Nazca plate 
    (near Chile) in South America since that area had pretty dense 
    data. This limited the size of the set of approximately 14000 
    examples. \\
    For kernel density (a), we found that the optimal width for a
    Laplacian kernel was \textbf{19}. \\
    Since kernel densities (b) and (c) are not proper densities,
    we found that by increasing the \(k\) value, the likelihoods
    would continue to decrease, so there is not optimal \(k\) value. \\
    For kernel density (d), despite it being a proper density, our 
    experiments did not converge (behaved the same as parts b and c).
    We think this may be due to programming error.
  \part % b
    We found the uniform density over the entire Earth to be
    \begin{equation*}
      P(x) = \frac{1}{2\pi^2} = \frac{1}{2 \cdot (180)^2}
    \end{equation*}
    This is a proper density since it integrates to \(1\) over all
    longitudes and latitudes. Let \(\theta_{\text{lat}}\) be the
    latitude and \(\theta_{\text{lon}}\) be the longitude. Longitudes
    range from \(-180^\circ\) to \(180^\circ\), while latitudes
    range from \(-90^\circ\) to \(90^\circ\).
    \begin{align*}
      \int\limits_{-180}^{180} \int\limits_{-90}^{90} \frac{1}{2(180)^2}
      \mathrm{d}\theta_{\text{lat}} \mathrm{d}\theta_{\text{lon}}
      &= \frac{1}{2(180)^2} \cdot (90 - (-90)) \cdot (180 - (-180)) \\
      &= \frac{1}{2(180)^2} \cdot (180) \cdot (360) \\
      &= 1
    \end{align*}
    We chose to use kernel density (a) since the other kernels did not
    converge. Our kernel density (a) gave consistent results whose
    likelihoods were higher than the uniform density (log-likelihood
    of \(-5\) compared to \(-11\) for the uniform density). This gave 
    us a weight of 1 for kernel density (a) and 0 for the uniform
    density
\end{question}
\end{document}
 
