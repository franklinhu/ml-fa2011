#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Franklin Hu, Sunil Pedapudi
\end_layout

\begin_layout Standard
Assignment 3
\end_layout

\begin_layout Enumerate
Entropy and Information Gain
\end_layout

\begin_deeper
\begin_layout Enumerate
Let us consider 
\begin_inset Formula 
\begin{align*}
B(q) & =-qlog(q)-(1-q)log(1-q)\\
\frac{dB}{dq} & =log(1-q)-log(q)\\
\frac{d^{2}B}{d^{2}q} & =\frac{1}{(q-1)q}
\end{align*}

\end_inset

Then, let 
\begin_inset Formula \ensuremath{q=\frac{p}{p+n}}

\end_inset

.
 We wish to find a maxima in order to demonstrate 
\begin_inset Formula \ensuremath{H(S)=B(\frac{p}{p+n})\leq1}

\end_inset

.
 Then,
\begin_inset Formula 
\begin{align*}
B'(\frac{p}{p+n}) & =log(1-\frac{p}{p+n})-log(\frac{p}{p+n})\\
 & =log(\frac{n}{p+n})-log(\frac{p}{p+n})\\
 & =log(\frac{\frac{n}{p+n}}{\frac{p}{p+n}})\\
 & =log(\frac{n}{p})=0
\end{align*}

\end_inset

This shows that there exists an optima where 
\begin_inset Formula \ensuremath{n=p}

\end_inset

 and we can verify that this point is a maximum by 
\begin_inset Formula 
\begin{align*}
B''(\frac{p}{p+n}) & =\frac{1}{(\frac{p}{p+n}-1)\frac{p}{p+n}}\\
\textnormal{Since \ensuremath{n=p},}\\
 & =\frac{1}{(0.5-1)0.5}<0
\end{align*}

\end_inset

 Therefore, there exists a maximum when 
\begin_inset Formula \ensuremath{n=p}

\end_inset

.
 Note that in this scenario,
\begin_inset Formula 
\begin{align*}
H(S) & =B(\frac{p}{p+p})=B(0.5)\\
 & =-0.5\cdot log(0.5)-0.5\cdot log(0.5)\\
 & =-log(0.5)\\
 & =1
\end{align*}

\end_inset

which shows that the equality is achieved under said constraint.
\end_layout

\end_deeper
\begin_layout Enumerate
Empirical Loss and Splits
\end_layout

\begin_layout Enumerate
Splitting continuous attributes
\end_layout

\begin_layout Enumerate
Majority voting
\end_layout

\begin_deeper
\begin_layout Enumerate
Suppose we have 
\begin_inset Formula $K$
\end_inset

 predictions 
\begin_inset Formula $X_{k}$
\end_inset

 which predict some true value 
\begin_inset Formula $\mu$
\end_inset

 with an error 
\begin_inset Formula $\epsilon_{k}\sim N(0,\sigma^{2})$
\end_inset

 so 
\begin_inset Formula $X_{k}=\mu+\epsilon_{k}$
\end_inset

.
 Then,
\begin_inset Formula 
\begin{align*}
\overline{X}= & \frac{1}{k}\cdot\sum_{k}X_{k}\\
\text{Var}(\overline{X})= & \text{Var}(\frac{1}{k}\cdot\sum_{k}X_{k})\\
= & \frac{1}{k^{2}}\text{Var}(\sum_{k}X_{k})
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

Since the errors are independent, 
\begin_inset Formula $\text{Var}(\sum_{k}X_{k})=\sum_{k}\text{Var}(X_{k})$
\end_inset


\begin_inset Formula 
\begin{align*}
\text{Var}(\overline{X})= & \frac{1}{k^{2}}\sum_{k}\text{Var}(X_{k})\\
= & \frac{1}{k^{2}}\cdot k\sigma^{2}\\
= & \frac{\sigma^{2}}{k}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Consider the case when the ensemble learning algorithm uses hypotheses that
 are entirely dependent.
 In this case, the error would be equal to 
\begin_inset Formula $\epsilon$
\end_inset

.
 Thus, the error of ensemble learning where the independence assumption
 is removed is never greater than 
\begin_inset Formula $\epsilon$
\end_inset

.
\end_layout

\end_deeper
\end_body
\end_document
