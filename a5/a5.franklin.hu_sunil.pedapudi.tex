\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\begin{document}
Franklin Hu, Sunil Pedapudi \\
CS 194-10 Machine Learning \\
Fall 2011 \\
Assignment 5 \\

\newcommand{\pr}{\mathbb{P}}

\begin{enumerate}
    \item Conjugate priors % 1
        \begin{enumerate}
            \item % 1a
                Let
                \begin{align*}
                    \textnormal{Likelihood:} \pr(x_1,\hdots,x_N) &=
                        \prod_i^N\lambda\mathrm{exp}(-\lambda x_i)\\
                    \textnormal{Prior:} \mathrm{gamma}(\lambda|\alpha,\beta) &=
                        \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}
                \end{align*}
                Then,
                \begin{align*}
                    \textnormal{Posterior:}\\
                    \pr(\lambda|x_1,\hdots,x_N) &=
                        \prod_i^N\lambda\mathrm{exp}(-\lambda x_i)\cdot
                        \frac{\beta^\alpha}{\Gamma(\alpha)}
                        \lambda^{\alpha-1}e^{-\beta\lambda}\\
                    &=  \lambda exp(\sum_i^N -\lambda x_i)
                        \lambda^{\alpha-1}e^{-\beta\lambda}
                        \frac{\beta^\alpha}{\Gamma(\alpha)}\\
                    &=  \lambda^{\alpha+N-1}exp(\sum_i^N -\lambda x_i - \beta\lambda)
                        \frac{\beta^\alpha}{\Gamma(\alpha)}\\
                    &=  \lambda^{\alpha+N-1}exp(-\lambda\sum_i^Nx_i + \beta)
                        \frac{\beta^\alpha}{\Gamma(\alpha)}
                        \sim \textnormal{gamma}(\alpha+N,\beta+\sum_i^Nx_i)
                \end{align*}
                Since the posterior also has a gamma distribution, we find the
                updates parameters are of the form \(\alpha+N\),\(\beta+\sum_i^Nx_i\).
                To find the prediction distribution,
                \begin{align*}
                    \pr(x_N+1|x_1,\hdots,x_N) &= \frac{\beta^\alpha}{\Gamma(\alpha)}
                        \int\lambda exp(-\lambda x_{N+1})
                        \cdot\lambda^{\alpha+N-1}exp(-\lambda(\beta+\sum_i^{N}x_i) d\lambda\\
                    &=  \frac{\beta^\alpha}{\Gamma(\alpha)} \int\lambda 
                        \cdot\lambda^{\alpha+N-1}exp(-\lambda(\beta+\sum_i^{N+1}x_i) d\lambda\\
                    &=  \frac{\beta^\alpha}{\Gamma(\alpha)} \int\lambda 
                        \cdot P(\lambda|\alpha+N,\beta+\sum_i^{N+1}x_i) d\lambda
               \end{align*}                        
                    We note that this describes the expectation for \(\lambda\) given a
                    gamma function \(\sim \textnormal{gamma}(\lambda|\alpha+N, \beta+\sum_i^{N+1}x_i)\).
                    Therefore,
               \begin{equation*}
                   \pr(x_N+1|x_1,\hdots,x_N) &= \frac{\alpha+N}{\beta+\sum_i^{N+1}x_i}
               \end{equation*}                   
                    
            \item % 1b
                Given the geometric distribution
                \begin{equation*}
                    P(X_i=k|\theta)= (1-\theta)^{k-1} \cdot \theta
                \end{equation*}
                and the beta distribution
                \begin{equation*}
                    \beta(\theta|a,b)= \alpha \theta^{a-1} (1-\theta)^{b-1}
                \end{equation*}
                we prove that the beta distribution is the conjugate prior
                for a likelihood with a geometric distribution.
                \begin{align*}
                    P(\theta|X)
                    &= P(\theta) \cdot P(X|\theta) \\
                    &= \alpha \cdot \theta^{a-1} \cdot (1-\theta)^{b-1} 
                        \cdot (1-\theta)^{k-1} \cdot \theta \\
                    &= \alpha \cdot \theta^{a} \cdot (1-\theta)^{b+k-2} \\
                    &= \beta(\theta|a+1, b+k-1) 
                \end{align*}
                The posterior has the form of a beta distribution so
                therefore the beta distribution is the conjugate prior for
                the geometric distribution. \\
                The update procedure for a beta posterior simply involves
                updating the \(a\) and \(b\) parameters
                \begin{align*}
                    a_{N+1} &\leftarrow a_N + 1 \\
                    b_{N+1} &\leftarrow b_N + k - 1
                \end{align*}
        \end{enumerate}
    \item Bayesian Naive Bayes % 2
    \item Logistic regression for credit scoring % 3
        \begin{enumerate}
            \item % 3a
                The data structure we chose for logistic regression is
                simply a class that keeps a set of weights for each of the
                features, has an update method for updating the weights,
                and draws predictions using the logit function
                \begin{equation*}
                    \text{Probability} = \frac{1}{1 + e^{-w^Tx}}
                \end{equation*}
            \item % 3b
                The likelihood is
                \begin{align*}
                    L(w) 
                        &= \frac{1}{1 + e^{-yw^Tx}} \\
                    \text{log likelihood} 
                        &= \text{log}\frac{1}{1 + e^{-yw^Tx}} \\
                        &= -\text{log} (1 + e^{-yw^Tx}) \\
                    \text{negative log likelihood}
                        &= \text{log} (1 + e^{-yw^Tx})
                \end{align*}
                Now we compute the gradient of the negative log likelihood
                \begin{align*}
                    \nabla \text{log} (1 + e^{-yw^Tx})
                        &= \nabla \text{log} \left( \frac{e^{yw^Tx} + 1}
                            {e^{yw^Tx}} \right) \\
                        &= \nabla \left( \text{log}(e^{yw^Tx} + 1) -
                            \text{log}(e^{yw^Tx}) \right) \\
                        &= \left( \frac{1}{e^{yw^Tx} + 1} \cdot e^{yw^Tx}
                            \cdot -yx_i \right) - \left( 
                            \frac{1}{e^{yw^Tx}} \cdot e^{yw^Tx} \cdot 
                            -yx_i \right) \\
                        &= yx_i - yx_i \cdot \frac{e^{yw^Tx}}
                            {e^{yw^Tx}+1} \\
                        &= yx_i - yx_i \cdot \left( \frac{e^{yw^Tx}+1}
                            {e^{yw^Tx}} \right)^{-1} \\
                        &= yx_i - yx_i \cdot (1 + e^{-yw^Tx})^{-1} \\
                        &= yx_i \left( 1 - \frac{1}{1 + e^{-yw^Tx}} \right)
                \end{align*}
                Therefore our update rule is simply
                \begin{align*}
                    w_{i+1} 
                        &= w_i + \alpha \cdot \nabla L \\
                        &= w_i + \alpha \cdot yx_i \cdot \left( 1 - 
                            \frac{1}{1 + e^{-yw^Tx}} \right)
                \end{align*}
        \end{enumerate}
\end{enumerate}
\end{document}
