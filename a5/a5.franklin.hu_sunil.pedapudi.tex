\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{algorithm}
\begin{document}
Franklin Hu, Sunil Pedapudi \\
CS 194-10 Machine Learning \\
Fall 2011 \\
Assignment 5 \\

\newcommand{\pr}{\mathbb{P}}

\begin{enumerate}
    \item Conjugate priors % 1
        \begin{enumerate}
            \item % 1a
                Let
                \begin{align*}
                    \textnormal{Likelihood:\ } \pr(x_1,\hdots,x_N) &=
                        \prod_i^N\lambda\exp\left(-\lambda x_i\right)\\
                    \textnormal{Prior:\ } \mathrm{gamma}(\lambda|\alpha,\beta) &=
                        \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}
                \end{align*}
                Then,
                \begin{align*}
                    \textnormal{Posterior:}\\
                    \pr(\lambda|x_1,\hdots,x_N) &=
                        \prod_i^N\lambda\exp\left(-\lambda x_i\right)\cdot
                        \frac{\beta^\alpha}{\Gamma(\alpha)}
                        \lambda^{\alpha-1}e^{-\beta\lambda}\\
                    &=  \lambda \exp\left(\sum_i^N -\lambda x_i\right)
                        \lambda^{\alpha-1}e^{-\beta\lambda}
                        \frac{\beta^\alpha}{\Gamma(\alpha)}\\
                    &=  \lambda^{\alpha+N-1}\exp\left(\sum_i^N -\lambda x_i - \beta\lambda\right)
                        \frac{\beta^\alpha}{\Gamma(\alpha)}\\
                    &=  \lambda^{\alpha+N-1}\exp\left(-\lambda\sum_i^Nx_i + \beta\right)
                        \frac{\beta^\alpha}{\Gamma(\alpha)}
                        \sim \textnormal{gamma}(\alpha+N,\beta+\sum_i^Nx_i)
                \end{align*}
                Since the posterior also has a gamma distribution, we find the
                updates parameters are of the form \(\alpha+N\),\(\beta+\sum_i^Nx_i\).
                To find the prediction distribution,
                \begin{align*}
                    \pr(x_N+1|x_1,\hdots,x_N) &\propto \frac{\beta^\alpha}{\Gamma(\alpha)}
                        \int\lambda \exp\left(-\lambda x_{N+1}\right)
                        \cdot\lambda^{\alpha+N-1}\exp\left(-\lambda(\beta+\sum_i^{N}x_i)\right) d\lambda\\
                    &\propto  \frac{\beta^\alpha}{\Gamma(\alpha)} \int\lambda 
                        \cdot\lambda^{\alpha+N-1}\exp\left(-\lambda(\beta+\sum_i^{N+1}x_i)\right) d\lambda\\
                    &\propto  \frac{\beta^\alpha}{\Gamma(\alpha)} \int\lambda 
                        \cdot P(\lambda|\alpha+N,\beta+\sum_i^{N+1}x_i) d\lambda
               \end{align*}                        
                    We note that this describes the expectation for \(\lambda\) given a
                    gamma function \(\sim \textnormal{gamma}(\lambda|\alpha+N, \beta+\sum_i^{N+1}x_i)\).
                    Therefore,
               \begin{equation*}
                   \pr(x_N+1|x_1,\hdots,x_N) \propto \frac{\alpha+N}{\beta+\sum_i^{N+1}x_i}
               \end{equation*}                   
            \item % 1b
                Given the geometric distribution
                \begin{equation*}
                    P(X_i=k|\theta)= (1-\theta)^{k-1} \cdot \theta
                \end{equation*}
                and the beta distribution
                \begin{equation*}
                    \beta(\theta|a,b)= \alpha \theta^{a-1} (1-\theta)^{b-1}
                \end{equation*}
                we prove that the beta distribution is the conjugate prior
                for a likelihood with a geometric distribution.
                \begin{align*}
                    P(\theta|X)
                    &= P(\theta) \cdot P(X|\theta) \\
                    &= \alpha \cdot \theta^{a-1} \cdot (1-\theta)^{b-1} 
                        \cdot (1-\theta)^{k-1} \cdot \theta \\
                    &= \alpha \cdot \theta^{a} \cdot (1-\theta)^{b+k-2} \\
                    &= \beta(\theta|a+1, b+k-1) 
                \end{align*}
                The posterior has the form of a beta distribution so
                therefore the beta distribution is the conjugate prior for
                the geometric distribution. \\
                The update procedure for a beta posterior simply involves
                updating the \(a\) and \(b\) parameters
                \begin{align*}
                    a_{N+1} &\leftarrow a_N + 1 \\
                    b_{N+1} &\leftarrow b_N + k - 1
                \end{align*}
            \item % 1c
              Given
              \begin{align*}
                \textnormal{Likelihood:\ } \pr(\mathbf{X}|\theta) \\
                \textnormal{Mixture prior:\ } \pr(\theta|\gamma_1,\hdots,\gamma_m)
              \end{align*}
              We wish to find the posterior via
              \begin{align*}
                \pr(\theta|\mathbf{X}) &= \pr(\theta|\gamma_1,\hdots,\gamma_m)
                \cdot \pr(\mathbf{X}|\theta) \\
                &=  \sum^M_{m=1}w_m\pr(\theta|\gamma_m)\prod_i^N\pr(x_i|\theta) \\
                &=  \sum^M_{m=1}w_m\pr(\theta|\gamma_m^+) \\
              \end{align*}
              This is to say that we can find a \(\gamma_m^+\) that renders
              \(\pr(\theta|\gamma_m^+)\) equal to
              \(\pr(\theta|\gamma_m)\prod_i^N\pr(x_i|\theta)\). The updates to
              \(\gamma\) may be done iteratively as
              \begin{align*}
                \pr(\theta|\gamma_m)\prod_i^N\pr(x_i|\theta)
                &=  \pr(\theta|\gamma_m)\pr(x_1|\theta)\hdots\pr(x_N|\theta) \\
                &=  \pr(\theta|\gamma_m^\prime)\pr(x_1|\theta)\hdots\pr(x_{N-1}|\theta) \\
                &=  \pr(\theta|\gamma_m^{\prime\prime})\pr(x_1|\theta)\hdots\pr(x_{N-1}|\theta) \\
                &\vdots \\
                &= \pr(\theta|\gamma_m^+) \\
              \end{align*}
              Since \(\pr(\theta|\gamma)\) is the conjugate prior for 
              \(\pr(\mathbf{X}|\theta\), we retain the mixture model 
              throughout the update.
            \item % 1d
              Given
              \begin{align*}
                \textnormal{Mixture likelihood:\ } \sum_{i=1}^N w_i\pr(x_i | \theta_i)) \\
                \textnormal{Prior:\ } \pr(\theta_1,\hdots,\theta_N|\gamma)
              \end{align*}
              We find the posterior via
              \begin{align*}
                \pr(\theta_1,\hdots,\theta_N|\mathbf{X})
                &=  \sum_{i=1}^N w_i\pr(x_i | \theta_i)\cdot\pr(\theta_i|\gamma) \\
                &=  \sum_{i=1}^N w_i\pr(x_i | \theta_i)\cdot\pr(\theta_i|\gamma)
              \end{align*}   
        \end{enumerate}
    \item Bayesian Naive Bayes % 2
        \begin{enumerate}
            \item % 2a
                Maximum likelihood learning chooses the hypothesis with the
                greatest likelihood where as Bayesian learning computes
                the weights over all hypotheses and uses a linear
                combination of their outputs. \\
                To use Bayesian learning, let us re-examine the computation
                of \(\pr(x_i|class\).
                \begin{equation*}
                  \pr(x_i|class)
                  &=  \int \pr(x_i|\lambda_{i,class})\pr(\lambda_i|class) d\lambda_{i,class}
                \end{equation*}
                We note that \(\pr(x_i|\lambda_{i,class})\) is an exponential distribution as
                that is the likelihood that attribute \(x_i\) belongs to a certain classification
                while \(\pr(\lambda_i|class)\) follows a gamma distribution as that is the prior
                given a certain classification. This renders
                \begin{equation*}
                  \pr(x_i|class)
                  &=  \int \left(\lambda_{i,class}\exp\left(-\lambda_{i,class}x_i\right)\right)
                  \cdot\left(\frac{\beta_i^{\alpha_i}}{\Gamma(\alpha)}\lambda^{\alpha_i-1}
                  \exp(-\beta_i\lambda_i)\right) d\lambda_{i,class}
                \end{equation*}
                We train across email samples to find the relevant \(\alpha_i,\beta_i\) by
                \begin{algorithmic}
                  \FOR{each sample} (x, y=class):
                    \FOR{i = 1 to D}:
                      \(\alpha_{i,y}\) \gets \(\alpha_{i,y}\) + 1
                      \(\geta_{i,y}\) \gets \(\beta_{i,y}\) + x_i
                    \ENDFOR
                  \ENDFOR
                \end{algorithmic}      
            \item % 2b
        \end{enumerate}
    \item Logistic regression for credit scoring % 3
        \begin{enumerate}
            \item % 3a
                The data structure we chose for logistic regression is
                simply a class that keeps a set of weights for each of the
                features, has an update method for updating the weights,
                and draws predictions using the logit function
                \begin{equation*}
                    \text{Probability} = \frac{1}{1 + e^{-w^Tx}}
                \end{equation*}
            \item % 3b
                The likelihood is
                \begin{align*}
                    L(w) 
                        &= \frac{1}{1 + e^{-yw^Tx}} \\
                    \text{log likelihood} 
                        &= \text{log}\frac{1}{1 + e^{-yw^Tx}} \\
                        &= -\text{log} (1 + e^{-yw^Tx}) \\
                    \text{negative log likelihood}
                        &= \text{log} (1 + e^{-yw^Tx})
                \end{align*}
                Now we compute the gradient of the negative log likelihood
                \begin{align*}
                    \nabla \text{log} (1 + e^{-yw^Tx})
                        &= \nabla \text{log} \left( \frac{e^{yw^Tx} + 1}
                            {e^{yw^Tx}} \right) \\
                        &= \nabla \left( \text{log}(e^{yw^Tx} + 1) -
                            \text{log}(e^{yw^Tx}) \right) \\
                        &= \left( \frac{1}{e^{yw^Tx} + 1} \cdot e^{yw^Tx}
                            \cdot -yx_i \right) - \left( 
                            \frac{1}{e^{yw^Tx}} \cdot e^{yw^Tx} \cdot 
                            -yx_i \right) \\
                        &= yx_i - yx_i \cdot \frac{e^{yw^Tx}}
                            {e^{yw^Tx}+1} \\
                        &= yx_i - yx_i \cdot \left( \frac{e^{yw^Tx}+1}
                            {e^{yw^Tx}} \right)^{-1} \\
                        &= yx_i - yx_i \cdot (1 + e^{-yw^Tx})^{-1} \\
                        &= yx_i \left( 1 - \frac{1}{1 + e^{-yw^Tx}} \right)
                \end{align*}
                Therefore our update rule is simply
                \begin{align*}
                    w_{i+1} 
                        &= w_i + \alpha \cdot \nabla L \\
                        &= w_i + \alpha \cdot yx_i \cdot \left( 1 - 
                            \frac{1}{1 + e^{-yw^Tx}} \right)
                \end{align*}
            \item % 3c
            \item % 3d
                The model gives a probability that the example is 1. Thus if
                the prediction is greater than 0.5, then we predict
                1; if the prediction is less than 0.5 we predict 0; and if
                the prediction is exactly 0.5 we flip a fair coin.
        \end{enumerate}
\end{enumerate}
\end{document}
