\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\begin{document}
Franklin Hu, Sunil Pedapudi \\
CS 194-10 Machine Learning \\
Fall 2011 \\
Assignment 5 \\

\begin{enumerate}
    \item Conjugate priors % 1
        \begin{enumerate}
            \item % 1a
            \item % 1b
                Given the geometric distribution
                \begin{equation*}
                    P(X_i=k|\theta)= (1-\theta)^{k-1} \cdot \theta
                \end{equation*}
                and the beta distribution
                \begin{equation*}
                    \beta(\theta|a,b)= \alpha \theta^{a-1} (1-\theta)^{b-1}
                \end{equation*}
                we prove that the beta distribution is the conjugate prior
                for a likelihood with a geometric distribution.
                \begin{align*}
                    P(\theta|X)
                    &= P(\theta) \cdot P(X|\theta) \\
                    &= \alpha \cdot \theta^{a-1} \cdot (1-\theta)^{b-1} 
                        \cdot (1-\theta)^{k-1} \cdot \theta \\
                    &= \alpha \cdot \theta^{a} \cdot (1-\theta)^{b+k-2} \\
                    &= \beta(\theta|a+1, b+k-1) 
                \end{align*}
                The posterior has the form of a beta distribution so
                therefore the beta distribution is the conjugate prior for
                the geometric distribution. \\
                The update procedure for a beta posterior simply involves
                updating the \(a\) and \(b\) parameters
                \begin{align*}
                    a_{N+1} &\leftarrow a_N + 1 \\
                    b_{N+1} &\leftarrow b_N + k - 1
                \end{align*}
        \end{enumerate}
    \item Bayesian Naive Bayes % 2
    \item Logistic regression for credit scoring % 3
        \begin{enumerate}
            \item % 3a
                The data structure we chose for logistic regression is
                simply a class that keeps a set of weights for each of the
                features, has an update method for updating the weights,
                and draws predictions using the logit function
                \begin{equation*}
                    \text{Probability} = \frac{1}{1 + e^{-w^Tx}}
                \end{equation*}
            \item % 3b
                The likelihood is
                \begin{align*}
                    L(w) 
                        &= \frac{1}{1 + e^{-yw^Tx}} \\
                    \text{log likelihood} 
                        &= \text{log}\frac{1}{1 + e^{-yw^Tx}} \\
                        &= -\text{log} (1 + e^{-yw^Tx}) \\
                    \text{negative log likelihood}
                        &= \text{log} (1 + e^{-yw^Tx})
                \end{align*}
                Now we compute the gradient of the negative log likelihood
                \begin{align*}
                    \nabla \text{log} (1 + e^{-yw^Tx})
                        &= \nabla \text{log} \left( \frac{e^{yw^Tx} + 1}
                            {e^{yw^Tx}} \right) \\
                        &= \nabla \left( \text{log}(e^{yw^Tx} + 1) -
                            \text{log}(e^{yw^Tx}) \right) \\
                        &= \left( \frac{1}{e^{yw^Tx} + 1} \cdot e^{yw^Tx}
                            \cdot -yx_i \right) - \left( 
                            \frac{1}{e^{yw^Tx}} \cdot e^{yw^Tx} \cdot 
                            -yx_i \right) \\
                        &= yx_i - yx_i \cdot \frac{e^{yw^Tx}}
                            {e^{yw^Tx}+1} \\
                        &= yx_i - yx_i \cdot \left( \frac{e^{yw^Tx}+1}
                            {e^{yw^Tx}} \right)^{-1} \\
                        &= yx_i - yx_i \cdot (1 + e^{-yw^Tx})^{-1} \\
                        &= yx_i \left( 1 - \frac{1}{1 + e^{-yw^Tx}} \right)
                \end{align*}
                Therefore our update rule is simply
                \begin{align*}
                    w_{i+1} 
                        &= w_i + \alpha \cdot \nabla L \\
                        &= w_i + \alpha \cdot yx_i \cdot \left( 1 - 
                            \frac{1}{1 + e^{-yw^Tx}} \right)
                \end{align*}
        \end{enumerate}
\end{enumerate}
\end{document}
